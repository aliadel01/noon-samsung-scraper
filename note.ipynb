{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8f7a49",
   "metadata": {},
   "source": [
    "# ðŸ›’ Web Scraping Samsung Products from Noon Egypt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501899e3",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "- **Project**: Scraping Samsung products from Noon Egypt  \n",
    "- **Goal**: Collect product details (name, price, rating, brand, etc.)  \n",
    "- **Tools**: Requests, BeautifulSoup, Selenium, Pandas  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad19b14",
   "metadata": {},
   "source": [
    "## Scraping Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be03026",
   "metadata": {},
   "source": [
    "### 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77162881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4539890",
   "metadata": {},
   "source": [
    "### 3. Collect Product Links (Requests + BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacbc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "links = []\n",
    "for i in range(1, 26):  # scrape first 25 pages\n",
    "    url = f\"https://www.noon.com/egypt-en/samsung/?page={i}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    for a in soup.find_all(\"a\", class_=\"PBoxLinkHandler_productBoxLink__fAJHN\"):\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            links.append(\"https://www.noon.com\" + href)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21536b4",
   "metadata": {},
   "source": [
    "### 4. Scrape Product Data (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27795092",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "start = 0  \n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "products_list = []\n",
    "\n",
    "for idx, link in enumerate(links[start:start+batch_size]):\n",
    "    try:\n",
    "        print(f\"Scraping product {idx+1}/{batch_size}\")\n",
    "        driver.get(link)\n",
    "        time.sleep(2)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # Extract product details\n",
    "        image = soup.find(\"img\", class_=\"imageMagnify\")\n",
    "        image = image[\"src\"] if image else None\n",
    "\n",
    "        name = soup.find(\"span\", class_=\"ProductTitle_title__vjUBn\")\n",
    "        name = name.get_text(strip=True) if name else None\n",
    "\n",
    "        price = soup.find(\"span\", class_=\"PriceOfferV2_priceNowText__fk5kK\")\n",
    "        price = price.get_text(strip=True) if price else None\n",
    "\n",
    "        rating = soup.find(\"span\", class_=\"RatingPreviewStarV2_text__XseM1\")\n",
    "        rating = rating.get_text(strip=True) if rating else None\n",
    "\n",
    "        number_of_ratings = soup.find(\"span\", class_=\"RatingPreviewStarV2_countText__OVzD2\")\n",
    "        number_of_ratings = number_of_ratings.get_text(strip=True) if number_of_ratings else None\n",
    "\n",
    "        breadcrumb_links = soup.find_all(\"a\", class_=\"Breadcrumb_breadcrumb__74hod\")\n",
    "        categories = [a.find_all(\"span\")[-1].get_text(strip=True) for a in breadcrumb_links]\n",
    "        category = categories[1] if len(categories) > 1 else None\n",
    "        subcategory = categories[-1] if len(categories) > 0 else None\n",
    "\n",
    "        brand = soup.find(\"span\", class_=\"BrandStoreCtaV2_textContent__6tPjk\")\n",
    "        brand = brand.get_text(strip=True) if brand else None\n",
    "\n",
    "        discount = soup.find(\"span\", class_=\"PriceOfferV2_profit__6gHFc\")\n",
    "        discount = discount.get_text(strip=True) if discount else None\n",
    "\n",
    "        sold_by = soup.find(class_=\"PartnerRatingsV2_soldBy__IOCr1\")\n",
    "        sold_by = sold_by.get_text(strip=True) if sold_by else None\n",
    "\n",
    "        best_seller_tag = soup.find(\"span\", class_=\"CategoryBestSellerRankV2_rank__jyrnN\")\n",
    "        best_seller_rank = best_seller_tag.get_text(strip=True) if best_seller_tag else None\n",
    "\n",
    "        category_tag = soup.find(\"span\", class_=\"CategoryBestSellerRankV2_category__ROWYW\")\n",
    "        best_seller_category = category_tag.get_text(strip=True) if category_tag else None\n",
    "\n",
    "        products_list.append((\n",
    "            name, rating, number_of_ratings, category, subcategory,\n",
    "            brand, price, discount, sold_by,\n",
    "            best_seller_rank, best_seller_category, image\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {idx + start}: {e}\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce56c7",
   "metadata": {},
   "source": [
    "### 5. Save Data to DataFrame and Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5134ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"name\", \"rating\", \"number_of_ratings\", \"category\", \"subcategory\",\n",
    "    \"brand\", \"price\", \"discount\", \"sold_by\",\n",
    "    \"best_seller_rank\", \"best_seller_category\", \"image\"\n",
    "]\n",
    "\n",
    "products_df = pd.DataFrame(products_list, columns=columns)\n",
    "\n",
    "products_df.to_csv(\"products.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb78bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92401172",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "\n",
    "âœ… Successfully scraped **Samsung products** from Noon.  \n",
    "âœ… Extracted details: *name, price, ratings, brand, category, etc.*  \n",
    "âœ… Data exported as **products.csv**.  \n",
    "\n",
    "### ðŸ”œ Next Steps  \n",
    "- ðŸ§¹ Add cleaning for missing values  \n",
    "- ðŸ“Š Perform deeper analysis (pricing trends, top-rated products)  \n",
    "- ðŸ“ˆ Create visualizations for better insights  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
